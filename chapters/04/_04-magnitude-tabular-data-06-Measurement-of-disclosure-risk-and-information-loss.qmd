## Measurement of disclosure risk and information loss 
### Disclosure risk

The disclosure risk for tabular data (especially magnitude) includes the assessment of: 
• primary risk,
• secondary risk.
The primary risk concerns the threat for direct identification of an unit resulting from too low frequency or existing of outliers according to the presented magnitude in the cell. The secondary risk assessment is necessary due to the fact that primary confidential cells in detailed tables may still not ensure sufficient protection against re-identification: together with single cells also sums for larger groups, i.e. the margins are computed. Then the protection of the primary sensitive cells can be undone, by some differencing. Therefore the risk of such an event should also be assessed. 

The key measure of dislosure risk in the case of magnitude tables is based on the $(n,l)$-dominance or $p\%$ rules. In many formal regulations or recommendations it is assumed $n=1$ and $l=75$. Very often $(n,l)$-dominance is combined with $k$-anonymity: a cell is regarded as unsafe if it violates the $k$-anonymity or the $(n,l)$-dominance. Therefore the dislosure risk can be measured as the share of cells violating the finally assumed principle. A broader discussion on an application of these rules to assess the disclosure risk is performed e.g. by Hundepool et al. (2012).

However, this assessment concerns only the primary risk. One should take into account the risk of secondary re-identification of unit. Antal, Shlomo and Elliot (2014), formulate four fundamental properties which should have an efficient disclosure risk measure: 
• small cell values should have higher disclosure risk than larger,
• uniformly distributed frequencies imply low disclosure risk,
• the more zero cells in the census table, the higher the disclosure risk,
• the risk measure should be bounded by 0 and 1.

In the currently investigated case of magnitude table one should add one more condition concerning the specificity of this type of data presentation. That is, the larger is the share of the largest contributors in a cell the higher is the disclosure risk.

Shlomo, Antal and Elliot (2015) proposed in this context measure based on the entropy. According to their approach, a high entropy indicates that the distribution across cells is uniform and a low entropy indicates mainly zeros in a row/column or table with a few non-zero cells. The fewer the number of non-zero cells, the more likely that attribute disclosure occurs. The entropy is given as
$$
H=-\sum_{i=1}^{k}{\frac{c_i}{n}\cdot\log{\frac{c_i}{n}}}, (4.6.1)
$$
where $c_i$ is the number of units contained in $i$-th cell and $n$ is the total number of cells in the table. The measure (4.6.1) is next normalized to the form
$$
1-\frac{H}{\log n}. (4.6.2)
$$

This approach, however, doesn't take our fifth condition into account. So, one can correct (4.6.2) to the form combining both aspects, e.g. in the following way: 
$$
r=1-\frac{1}{2}\left(\frac{H}{\log n}+\frac{1}{n}\sum_{i=1}^n{\frac{x_{i(\text{max})}}{\sum_{j\in C_i}{x_j}}}\right),
$$
where $x_{i(\text{max})}$ is the share of the largest contributors to the cell $C_i$ and $x_j$ is the contribution of the $j$-th unit to it. This measure takes value from $[0,1]$ and is easy interpretable. 

The disclosure risk assessment can be also adjusted to the specificity of currently used SDC method. For instance, Enderle, Giessing and Tent (2020) have proposed an original risk measure for continous data perturbed using the Cell Key approach. It assumes that an intruder can know the noise parameter and the maximum deviation parameter. Then he/she can determine the feasibility intervals for original internal and margin cells using the noisy values. The risk estimate is then based on the sum of products of cumulated probabilities resulting from entries of p-table and empirical probabilities for ratio of noise parameters. 

### Information loss

The basis of assessment of information loss for tables are differences between values of cells determined using microdata after application of SDC methods (or perturbed/hidden during creation of tables) and relevant values obtained on the basis of the original data. Let $T_0$ denote a table generated using original microdata and $T_1$ - analogous table created on the basis of perturbed relevant microdata (or in which original cell vaules were pretrurbed - post-tabular pertrurbation). Denote by $T_l(c)$ the value of cell $c$ of table $T_l$, $l=0,1$.

The following metrics are commonly recommended for determining measures of information loss due to application of SDC process in magnitude tables:
• absolute deviation: $|T_1(c)-T_0(c)|$,
• relative absolute deviation: $\frac{|T_1(c)-T_0(c)|}{T_0(c)}$, 
• absolute deviation of square roots: $|\sqrt{T_1(c)}-\sqrt{T_0(c)}|$.
for any $c$.

As one can observe, the absolute deviation of square roots has a sense in practice only if the cell values are nonnegative. However, this is the most common situation. Otherwise, $\sqrt{T_l(c)}$ can be replaced with $-\sqrt{|T_l|}$, $l=0,1$. 

Using the metrics given above one can define complex measures of information loss for a given aggregate $A$ (it can be the whole table or a specific subset of table cells, referring to, for example, the same spatial unit - such as population numbers by age groups for a LAU 1 unit). The first of these measures is the average absolute deviation - the mean of absolute differences between cell values in original and modified tables:
$$
\textmd{AD}(A)=\frac{\sum_{c\in A}{|T_1(c)-T_0(c)|}}{n_A},
$$
where $n_A$ is the number of cells included in the aggregate $A$.

One can also propose to use in this context the sum of relative absolute deviations, i.e. the the sum of relative differences between cell values in both tables:
$$
\textmd{RAD}(A)=\sum_{c\in A}{\frac{|T_1(c)-T_0(c)|}{T_0(c)}}.
$$

The last - but not least - measure which we would like to present here is the measure based on absolute differences between square roots from cell values in original nad perturbed tables using the formula of Hellinger's distance (proposed in 1909 by German mathematician Ernst Hellinger (1883-1950)):
$$
\textmd{HD}(A)=\sqrt{\frac{1}{2}\sum_{c\in A}{\left(\sqrt{T_1(c)}-\sqrt{T_0(c)}\right)^2}}.
$$

However, there may be missing data in the tables. When perturbative SDC methods are applied, these gaps will result mainly from missing data occurring in the original microdata being a basis of construction of tables. In this case, during computation of cell values one can omit these gaps or earlier make an imputation of them. The only inconvenience may appear when there are no data concerning the categories defined by the cell. Then one must either resign from a given structure of the table (e.g. by combining some of the original categories into another, more coarse one), or the measure of loss should be based on the measurement of loss at the level of microdata corresponding to this cell, performed in the manner described in chapter 3.
 
More difficult is the situation where non-perturbative post-tabular SDC methods are used. It leads to suppression of values of some cells. When primary suppression is used, for making a comparison of tables before and after SDC process one should simulate a table, in which for missing cells their values are imputed. However, in the case of secondary suppression of cells the information loss will be expressed as the sum of costs incurred due to it. The problem is whether the weight of each cell in the table is the same, or whether cells with a higher value have a higher weight. In practice, suppression of too many cells with high values can significantly decrease the utility of disseminated data. The issue of information loss can be variously expressed, depending on preferences and needs of a user. In this way, it is possible to influence the operation of the algorithm for selecting cells for secondary suppression. Hundepool et al. (2012) indicate most common criteria taken into account when the cost function for cell suppression is defined:
• the same wages for all cells - for minimalization of the number of secondary suppressed cells,
• number of units in aggregate represented by a cell - leads to looking for possibilities of suppression of only such cells which will represent jointly as small number of units as possible,
• cell value - an optimal solution is leaving in publication as many cells with higher values as possible. 

If a strong asymmetry of data distribution preferring preferring to leave the cells with the highest value unsuppressed may lead to too large imbalance for the cost fuction. In this case a tranformation of the cost function is reommended. One of possible - and often applied - solutions is here the power transformation (cf. Box and Cox (1964)):
$$
y=\begin{cases}
x^\lambda&\textmd{for}\;\lambda\ne 0,\cr
\log(x)&\textmd{for}\;\lambda=0.
\end{cases}
$$

### References
Antal, L., Shlomo, N., & Elliot, M. (2014). Measuring disclosure risk with entropy in population based frequency tables. In *Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2014, Ibiza, Spain, September 17-19, 2014. Proceedings* (pp. 62-78). Springer International Publishing.

Box, G. E., Cox, D. R. (1964), An analysis of transformations. *Journal of the Royal Statistical Society. Series B (Methodological)*, 26:211–252.

Enderle, T., Giessing, S., & Tent, R. (2020). Calculation of risk probabilities for the cell key method. In: *Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2020, Tarragona, Spain, September 23–25, 2020, Proceedings* (pp. 151-165). Springer International Publishing.

Hundepool. A., Domingo-Ferrer. J., Franconi, L., Giessing S., Nordholt, E.S., Spicer, K. and de Wolf, P-P. (2012), *Statistical Disclosure Control*, series: Wiley Series in Survey Methodology, John Wiley & Sons, Ltd., Chichester.

Shlomo, N., Antal, L., & Elliot, M. (2015). Measuring disclosure risk and data utility for flexible table generators. *Journal of Official Statistics*, 31(2), 305-324.
